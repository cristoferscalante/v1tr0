---
title: "Fundamentos del Deep Learning y las Redes Neuronales"
date: 2025-10-02
author: "Alvaro Efren BolaÃ±os Scalante"
tags: ["deep-learning", "redes-neuronales", "matemÃ¡ticas", "neurona-artificial"]
series: "Deep Learning desde Cero"
part: 2
---

# Fundamentos del Deep Learning y las Redes Neuronales

## ğŸ§  Â¿QuÃ© es Deep Learning?

El **Aprendizaje Profundo** o **Deep Learning** es una subdisciplina del aprendizaje automÃ¡tico que se distingue por el uso de algoritmos basados en **redes neuronales profundas**. Estas redes replican, de manera conceptual, la interconexiÃ³n de las neuronas biolÃ³gicas en el cerebro humano.

Como seÃ±ala VerÃ³nica HernÃ¡ndez (BBVA AI Factory):

> "El 'deep learning' se centra en crear algoritmos que pueden aprender a travÃ©s de redes neuronales profundas"

Las redes neuronales profundas crean una jerarquÃ­a de conceptos donde las representaciones mÃ¡s abstractas se construyen a partir de otras mÃ¡s simples.

## ğŸ”¬ Estructura de una Red Neuronal Artificial

Una **red neuronal artificial** es un modelo computacional compuesto por nodos interconectados, denominados "neuronas", que trabajan en conjunto para procesar y analizar datos.

**MatemÃ¡ticamente:** Una red neuronal transforma un vector de entradas en un vector de salidas mediante operaciones matemÃ¡ticas organizadas en capas.

## ğŸ§® Componentes MatemÃ¡ticos de una Neurona

### 1. CaracterÃ­sticas de entrada (X)

Vector de entrada: **X âˆˆ â„â¿**

```python
# Ejemplo: imagen de 28x28 pÃ­xeles
X = [0.2, 0.8, 0.3, ..., 0.9]  # 784 valores
```

### 2. Pesos (W)

Matriz que pondera la importancia de cada conexiÃ³n.

```python
W = [[0.5, -0.3, 0.8],
     [0.2, 0.7, -0.1],
     ...]
```

### 3. Sesgos (b)

ParÃ¡metro escalar que proporciona flexibilidad al modelo.

```python
b = 0.5
```

## ğŸ“Š La FunciÃ³n de ActivaciÃ³n Sigmoide

La operaciÃ³n central de una neurona involucra una **funciÃ³n de activaciÃ³n no lineal**.

### FunciÃ³n Sigmoide Ïƒ(z)

**FÃ³rmula:**
```
Ïƒ(z) = 1 / (1 + eâ»á¶»)
```

**Comportamiento:**
- Entrada grande y positiva â†’ salida â‰ˆ 1 (neurona activa)
- Entrada grande y negativa â†’ salida â‰ˆ 0 (neurona inactiva)
- Rango de salida: [0, 1]

```python
import numpy as np

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# Ejemplo
z = np.array([-2, 0, 2])
print(sigmoid(z))  # [0.119, 0.5, 0.881]
```

### Ajuste con ParÃ¡metros

En la expresiÃ³n **Ïƒ(3(zâˆ’5))**:
- Factor **3**: Intensifica la pendiente (transiciÃ³n mÃ¡s abrupta)
- Desplazamiento **-5**: Mueve el punto de transiciÃ³n hacia la derecha

## ğŸ—ï¸ Redes Multicapa

En una red multicapa, las neuronas se organizan en capas sucesivas:

```
Capa Entrada â†’ Capa Oculta 1 â†’ Capa Oculta 2 â†’ Capa Salida
```

### FÃ³rmula de ActivaciÃ³n

```
aáµ¢[Ë¡] = Ïƒ(Î£(wáµ¢â±¼[Ë¡] Â· aâ±¼[Ë¡â»Â¹]) + báµ¢[Ë¡])
```

**Donde:**
- **aâ±¼[Ë¡â»Â¹]**: ActivaciÃ³n de la neurona j de la capa anterior
- **wáµ¢â±¼[Ë¡]**: Peso de conexiÃ³n
- **báµ¢[Ë¡]**: Sesgo de la neurona i

```python
# Ejemplo conceptual
def calcular_activacion(a_anterior, W, b):
    z = np.dot(W, a_anterior) + b
    return sigmoid(z)
```

## ğŸ¯ El Proceso de Aprendizaje

Para que la red "aprenda", necesitamos:

1. **Mecanismo para medir** predicciones â†’ **FunciÃ³n de Costo**
2. **MÃ©todo para ajustar** parÃ¡metros â†’ **Descenso de Gradiente**
3. **Algoritmo para calcular** gradientes â†’ **RetropropagaciÃ³n**

Estos conceptos se explorarÃ¡n en detalle en el prÃ³ximo blog.

## ğŸŒŸ Ventajas del Deep Learning

- ğŸ“ˆ **Escalabilidad**: Mejora con mÃ¡s datos
- ğŸ”„ **Aprendizaje automÃ¡tico de caracterÃ­sticas**: No requiere ingenierÃ­a manual
- ğŸ¨ **Versatilidad**: Aplica a imÃ¡genes, texto, audio, video
- ğŸš€ **Estado del arte**: Mejores resultados en muchas tareas

## ğŸ“Š Arquitectura Visual

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Capa      â”‚â”€â”€â”€â”€â–¶â”‚   Capas     â”‚â”€â”€â”€â”€â–¶â”‚   Capa      â”‚
â”‚   Entrada   â”‚     â”‚   Ocultas   â”‚     â”‚   Salida    â”‚
â”‚  (784)      â”‚     â”‚  (128, 64)  â”‚     â”‚    (10)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â–²                     â”‚                    â”‚
     â”‚                     â–¼                    â–¼
  Imagen              ExtracciÃ³n          ClasificaciÃ³n
  28x28               de Features         (0-9)
```

---

## ğŸ“š Referencias

- Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep learning*. MIT Press.
- IBM. (s.f.). Â¿QuÃ© es Deep learning? https://www.ibm.com/es-es/topics/deep-learning
- Higham, C. F., & Higham, D. J. (2019). Deep learning: An introduction for applied mathematicians.

---

**Anterior:** [â† Blog 1: IntroducciÃ³n al Machine Learning](01-introduccion-al-aprendizaje-automatico.md)

**PrÃ³ximo:** [Blog 3: El Proceso de Entrenamiento â†’](03-proceso-entrenamiento-redes-neuronales.md)