---
title: "AplicaciÃ³n PrÃ¡ctica: ConstrucciÃ³n de una Red Neuronal con Keras"
date: "2025-10-02"
author: "Alvaro Efren BolaÃ±os Scalante"
tags: ["keras", "tensorflow", "python", "prÃ¡ctica", "mnist"]
series: "Deep Learning desde Cero"
part: 4
excerpt: "Aprende a construir, entrenar y evaluar una red neuronal desde cero usando Keras y TensorFlow con el dataset MNIST."
---

# AplicaciÃ³n PrÃ¡ctica: ConstrucciÃ³n de una Red Neuronal con Keras

## ğŸ¯ IntroducciÃ³n

Tras explorar los fundamentos matemÃ¡ticos, es hora de **pasar de la teorÃ­a a la prÃ¡ctica**. Utilizaremos **Keras**, una biblioteca de alto nivel para Deep Learning, para construir, entrenar y evaluar una red neuronal.

## ğŸ“¦ Caso de Estudio: ClasificaciÃ³n de DÃ­gitos (MNIST)

Trabajaremos con el dataset **MNIST**: 70,000 imÃ¡genes de dÃ­gitos escritos a mano (0-9).

```
ğŸ“Š Dataset MNIST:
   - ImÃ¡genes: 28Ã—28 pÃ­xeles en escala de grises
   - Entrenamiento: 60,000 imÃ¡genes
   - Prueba: 10,000 imÃ¡genes
   - Clases: 10 (dÃ­gitos 0-9)
```

---

## ğŸ› ï¸ Paso 1: PreparaciÃ³n y Preprocesado de Datos

### 1.1 InstalaciÃ³n de Dependencias

```bash
pip install tensorflow numpy matplotlib scikit-learn
```

### 1.2 Importar Bibliotecas

```python
import numpy as np
import matplotlib.pyplot as plt
from tensorflow import keras
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.utils import to_categorical
```

### 1.3 Cargar el Dataset

```python
# Cargar MNIST
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

print(f"Forma de x_train: {x_train.shape}")  # (60000, 28, 28)
print(f"Forma de y_train: {y_train.shape}")  # (60000,)
print(f"Forma de x_test: {x_test.shape}")    # (10000, 28, 28)
```

### 1.4 Visualizar Ejemplos

```python
# Visualizar las primeras 9 imÃ¡genes
plt.figure(figsize=(10, 10))
for i in range(9):
    plt.subplot(3, 3, i+1)
    plt.imshow(x_train[i], cmap='gray')
    plt.title(f"Etiqueta: {y_train[i]}")
    plt.axis('off')
plt.tight_layout()
plt.show()
```

### 1.5 NormalizaciÃ³n

Escalar pÃ­xeles de [0, 255] â†’ [0, 1]:

```python
# Normalizar
x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0

print(f"Rango de valores: [{x_train.min()}, {x_train.max()}]")
# Salida: [0.0, 1.0]
```

### 1.6 CodificaciÃ³n One-Hot

Convertir etiquetas en vectores:

```python
# One-hot encoding
y_train_encoded = to_categorical(y_train, num_classes=10)
y_test_encoded = to_categorical(y_test, num_classes=10)

print(f"Etiqueta original: {y_train[0]}")
print(f"Etiqueta codificada:\n{y_train_encoded[0]}")

# Salida:
# Etiqueta original: 5
# Etiqueta codificada: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
```

---

## ğŸ—ï¸ Paso 2: DefiniciÃ³n y CompilaciÃ³n del Modelo

### 2.1 Arquitectura de la Red

```python
model = Sequential([
    # Aplanar imagen 28x28 a vector 784
    Flatten(input_shape=(28, 28)),
    
    # Primera capa oculta: 128 neuronas con ReLU
    Dense(128, activation='relu'),
    
    # Segunda capa oculta: 64 neuronas con ReLU
    Dense(64, activation='relu'),
    
    # Capa de salida: 10 neuronas con Softmax
    Dense(10, activation='softmax')
])
```

**Arquitectura Visual:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Input     â”‚  28Ã—28 imagen
â”‚  (28, 28)   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚  Flatten    â”‚  Transforma a vector 784
â”‚   (784)     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚  Dense      â”‚  128 neuronas + ReLU
â”‚   (128)     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚  Dense      â”‚  64 neuronas + ReLU
â”‚    (64)     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚  Dense      â”‚  10 neuronas + Softmax
â”‚    (10)     â”‚  (probabilidades por clase)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 Resumen del Modelo

```python
model.summary()
```

**Salida:**

```
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten (Flatten)            (None, 784)               0         
_________________________________________________________________
dense (Dense)                (None, 128)               100480    
_________________________________________________________________
dense_1 (Dense)              (None, 64)                8256      
_________________________________________________________________
dense_2 (Dense)              (None, 10)                650       
=================================================================
Total params: 109,386
Trainable params: 109,386
Non-trainable params: 0
```

### 2.3 Compilar el Modelo

```python
model.compile(
    optimizer='adam',                    # Optimizador Adam
    loss='categorical_crossentropy',     # Para clasificaciÃ³n multiclase
    metrics=['accuracy']                 # MÃ©trica a monitorear
)
```

**Opciones de optimizadores:**
- `'sgd'`: Descenso de gradiente estocÃ¡stico
- `'adam'`: Adam (recomendado para principiantes)
- `'rmsprop'`: RMSprop

---

## ğŸš€ Paso 3: Entrenamiento del Modelo

### 3.1 Entrenar

```python
history = model.fit(
    x_train, 
    y_train_encoded,
    epochs=10,              # NÃºmero de Ã©pocas
    batch_size=32,          # TamaÃ±o del mini-lote
    validation_split=0.2,   # 20% para validaciÃ³n
    verbose=1               # Mostrar progreso
)
```

**Salida durante el entrenamiento:**

```
Epoch 1/10
1500/1500 [======] - 3s 2ms/step - loss: 0.2645 - accuracy: 0.9234 - val_loss: 0.1345 - val_accuracy: 0.9612
Epoch 2/10
1500/1500 [======] - 2s 1ms/step - loss: 0.1123 - accuracy: 0.9665 - val_loss: 0.1012 - val_accuracy: 0.9701
...
```

### 3.2 Visualizar el Aprendizaje

```python
# Extraer historial
train_loss = history.history['loss']
val_loss = history.history['val_loss']
train_acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

# Crear grÃ¡ficas
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

# GrÃ¡fica de pÃ©rdida
ax1.plot(train_loss, label='Entrenamiento', marker='o')
ax1.plot(val_loss, label='ValidaciÃ³n', marker='s')
ax1.set_title('PÃ©rdida durante el Entrenamiento')
ax1.set_xlabel('Ã‰poca')
ax1.set_ylabel('PÃ©rdida')
ax1.legend()
ax1.grid(True)

# GrÃ¡fica de precisiÃ³n
ax2.plot(train_acc, label='Entrenamiento', marker='o')
ax2.plot(val_acc, label='ValidaciÃ³n', marker='s')
ax2.set_title('PrecisiÃ³n durante el Entrenamiento')
ax2.set_xlabel('Ã‰poca')
ax2.set_ylabel('PrecisiÃ³n')
ax2.legend()
ax2.grid(True)

plt.tight_layout()
plt.show()
```

---

## ğŸ“Š Paso 4: EvaluaciÃ³n del Modelo

### 4.1 Evaluar en Datos de Prueba

```python
test_loss, test_accuracy = model.evaluate(x_test, y_test_encoded, verbose=0)

print(f"\n{'='*50}")
print(f"ğŸ“Š RESULTADOS EN DATOS DE PRUEBA")
print(f"{'='*50}")
print(f"PÃ©rdida (Loss): {test_loss:.4f}")
print(f"PrecisiÃ³n (Accuracy): {test_accuracy:.4f} ({test_accuracy*100:.2f}%)")
print(f"{'='*50}")
```

**Salida esperada:**

```
==================================================
ğŸ“Š RESULTADOS EN DATOS DE PRUEBA
==================================================
PÃ©rdida (Loss): 0.0856
PrecisiÃ³n (Accuracy): 0.9741 (97.41%)
==================================================
```

### 4.2 Hacer Predicciones

```python
# Predecir las primeras 10 imÃ¡genes del test
predictions = model.predict(x_test[:10])

# Convertir probabilidades a clases
predicted_classes = np.argmax(predictions, axis=1)
true_classes = np.argmax(y_test_encoded[:10], axis=1)

# Mostrar resultados
print("\nğŸ”® PREDICCIONES:")
print(f"{'Ãndice':<10}{'Predicho':<12}{'Real':<10}{'Â¿Correcto?'}")
print("-" * 45)
for i in range(10):
    correcto = "âœ…" if predicted_classes[i] == true_classes[i] else "âŒ"
    print(f"{i:<10}{predicted_classes[i]:<12}{true_classes[i]:<10}{correcto}")
```

### 4.3 Visualizar Predicciones

```python
plt.figure(figsize=(15, 6))
for i in range(10):
    plt.subplot(2, 5, i+1)
    plt.imshow(x_test[i], cmap='gray')
    
    pred = predicted_classes[i]
    true = true_classes[i]
    color = 'green' if pred == true else 'red'
    
    plt.title(f"Pred: {pred}\nReal: {true}", color=color)
    plt.axis('off')
plt.tight_layout()
plt.show()
```

---

## ğŸ’¾ Paso 5: Guardar y Cargar el Modelo

### 5.1 Guardar el Modelo

```python
# Guardar el modelo completo
model.save('modelo_mnist.h5')
print("âœ… Modelo guardado como 'modelo_mnist.h5'")

# O guardar solo los pesos
model.save_weights('pesos_mnist.h5')
```

### 5.2 Cargar el Modelo

```python
# Cargar modelo completo
modelo_cargado = keras.models.load_model('modelo_mnist.h5')

# Verificar que funciona
test_loss_cargado, test_acc_cargado = modelo_cargado.evaluate(
    x_test, y_test_encoded, verbose=0
)
print(f"PrecisiÃ³n del modelo cargado: {test_acc_cargado:.4f}")
```

---

## ğŸ¨ CÃ³digo Completo

```python
"""
Red Neuronal para ClasificaciÃ³n de DÃ­gitos MNIST
Autor: Alvaro Efren BolaÃ±os Scalante
"""

import numpy as np
import matplotlib.pyplot as plt
from tensorflow import keras
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.utils import to_categorical

# ============================================
# 1. CARGAR Y PREPROCESAR DATOS
# ============================================
print("ğŸ“¦ Cargando datos...")
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

# Normalizar
x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0

# One-hot encoding
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)

print(f"âœ… Datos cargados: {x_train.shape[0]} entrenamiento, {x_test.shape[0]} prueba")

# ============================================
# 2. DEFINIR MODELO
# ============================================
print("\nğŸ—ï¸ Construyendo modelo...")
model = Sequential([
    Flatten(input_shape=(28, 28)),
    Dense(128, activation='relu'),
    Dense(64, activation='relu'),
    Dense(10, activation='softmax')
])

model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

model.summary()

# ============================================
# 3. ENTRENAR
# ============================================
print("\nğŸš€ Entrenando modelo...")
history = model.fit(
    x_train, y_train,
    epochs=10,
    batch_size=32,
    validation_split=0.2,
    verbose=1
)

# ============================================
# 4. EVALUAR
# ============================================
print("\nğŸ“Š Evaluando modelo...")
test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)
print(f"\nPrecisiÃ³n en test: {test_acc:.4f} ({test_acc*100:.2f}%)")

# ============================================
# 5. VISUALIZAR
# ============================================
# GrÃ¡ficas de aprendizaje
plt.figure(figsize=(14, 5))

plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Entrenamiento')
plt.plot(history.history['val_loss'], label='ValidaciÃ³n')
plt.title('PÃ©rdida')
plt.xlabel('Ã‰poca')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)

plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Entrenamiento')
plt.plot(history.history['val_accuracy'], label='ValidaciÃ³n')
plt.title('PrecisiÃ³n')
plt.xlabel('Ã‰poca')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.savefig('entrenamiento.png')
plt.show()

print("\nâœ… Â¡Entrenamiento completado!")
```

---

## ğŸ¯ Ejercicios Propuestos

1. **Cambiar la arquitectura**: Prueba con mÃ¡s/menos capas y neuronas
2. **Probar otros optimizadores**: `'sgd'`, `'rmsprop'`
3. **Ajustar hiperparÃ¡metros**: learning rate, batch size
4. **Usar Fashion-MNIST**: Dataset de prendas de ropa
5. **Agregar regularizaciÃ³n**: Dropout para evitar overfitting

```python
# Ejemplo con Dropout
from tensorflow.keras.layers import Dropout

model = Sequential([
    Flatten(input_shape=(28, 28)),
    Dense(128, activation='relu'),
    Dropout(0.5),  # 50% de dropout
    Dense(64, activation='relu'),
    Dropout(0.3),
    Dense(10, activation='softmax')
])
```

---

## ğŸ“š Referencias

- Torres, J. (2020). *Python Deep Learning: IntroducciÃ³n prÃ¡ctica con Keras y TensorFlow 2*. Marcombo.
- DocumentaciÃ³n de Keras: https://keras.io
- TensorFlow Tutorials: https://www.tensorflow.org/tutorials

---

**Anterior:** [â† Blog 3: Proceso de Entrenamiento](03-redes-neuronales.md)

**PrÃ³ximo:** [Blog 5: EvaluaciÃ³n y DesafÃ­os â†’](05-aplicaciones-ml.md)
